{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#misc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch\n",
    "from torch.utils.data import DataLoader as torch_dl\n",
    "from torch.utils.data import Dataset\n",
    "from torch import  nn\n",
    "from torch import optim\n",
    "from torch.nn.init import *\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Route15.csv', index_col=0)\n",
    "train['TT_next30'] = train['TT'].shift(-6)\n",
    "train, val = train[288*(365+60):288*(365+90+100)], train[288*(365+90+100):-6]\n",
    "#test = pd.read_csv('input/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, cats):\n",
    "    \"\"\"\n",
    "    map the string data to integer id for calculate embedding\n",
    "    \"\"\"\n",
    "    data = data.fillna('missing') # replace nan with str \"missing\"\n",
    "    data = data.replace('-99', 'missing')\n",
    "    # map data[cat] to integer ids\n",
    "    for cat in cats:\n",
    "        data[cat] = data[cat].astype('category').cat.codes # cat: some utilities for categorical\n",
    "    # for those who are already integer ids, don't have to change\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmbeddingDataPreprocess(data, cats, inplace =True):\n",
    "    ### Each categorical column should have indices as values \n",
    "    ### Which will be looked up at embedding matrix and used in modeling\n",
    "    ### Make changes inplace\n",
    "    if inplace:\n",
    "        for c in cats:\n",
    "            # same as data[c].replace({val:i for i, val in zip(range(len(data[c].unique())), data_copy[c].unique())})\n",
    "            data[c].replace({val:i for i, val in enumerate(data[c].unique())}, inplace=True)\n",
    "        return data\n",
    "    else:\n",
    "        data_copy = data.copy()\n",
    "        for c in cats:\n",
    "            data_copy[c].replace({val:i for i, val in enumerate(data_copy[c].unique())}, inplace=True)\n",
    "        return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embs_dims(data, cats):\n",
    "    # get # unique categories of each category\n",
    "    cat_sz = [len(data[c].unique()) for c in cats]\n",
    "    return [(c, min(50, (c+1)//2)) for c in cat_sz] # we don't want the embedding vector too long (over 50) so use min(50, (c+1)//2)\n",
    "def emb_init(x):\n",
    "    x = x.weight.data\n",
    "    sc = 2/(x.size(1)+1)\n",
    "    x.uniform_(-sc,sc)\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    ### This dataset will prepare inputs cats, conts and output y \n",
    "    ### To be feed into our mixed input embedding fully connected NN model \n",
    "    ### Stacks numpy arrays to create nxm matrices where n = rows, m = columns\n",
    "    ### Gives y 0 if not specified\n",
    "    def __init__(self, cats, conts, y):\n",
    "        n = len(cats[0]) if cats else len(conts[0])\n",
    "        self.cats = np.stack(cats, 1).astype(np.int64) if cats else np.zeros((n,1))\n",
    "        self.conts = np.stack(conts, 1).astype(np.float32) if conts else np.zeros((n,1))\n",
    "        self.y = np.zeros((n,1)) if y is None else y[:,None].astype(np.float32)\n",
    "        \n",
    "    def __len__(self): return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.cats[idx], self.conts[idx], self.y[idx]]\n",
    "    \n",
    "    @classmethod\n",
    "    def from_data_frames(cls, df_cat, df_cont, y=None):\n",
    "        cat_cols = [c.values for n,c in df_cat.items()]\n",
    "        cont_cols = [c.values for n,c in df_cont.items()]\n",
    "        return cls(cat_cols, cont_cols, y)\n",
    "\n",
    "    @classmethod\n",
    "    def from_data_frame(cls, df, cat_flds, y=None):\n",
    "        return cls.from_data_frames(df[cat_flds], df.drop(cat_flds, axis=1), y)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We will keep this for fastai compatibility\n",
    "class ModelData():\n",
    "    def __init__(self, path, trn_dl, val_dl, test_dl=None):\n",
    "        self.path,self.trn_dl,self.val_dl,self.test_dl = path,trn_dl,val_dl,test_dl\n",
    "        \n",
    "class EmbeddingModelData(ModelData):\n",
    "    ### This class provides training and validation dataloaders\n",
    "    ### Which we will use in our model\n",
    "    \n",
    "    def __init__(self, path, trnx_ds, val_ds, bs, test_ds=None):\n",
    "        test_dl = DataLoader(test_ds, bs, shuffle=False, num_workers=1) if test_ds is not None else None\n",
    "        super().__init__(path, torch_dl(trnx_ds, batch_size=bs, shuffle=True, num_workers=1)\n",
    "                         ,torch_dl(val_ds, batch_size=bs, shuffle=True, num_workers=1), test_ds)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_data_frames(cls, path, trn_df, val_df, trn_y, val_y, cat_flds, bs, test_df=None):\n",
    "        test_ds = EmbeddingDataset.from_data_frame(test_df, cat_flds) if test_df is not None else None\n",
    "        return cls(path, EmbeddingDataset.from_data_frame(trn_df, cat_flds, trn_y),\n",
    "                    EmbeddingDataset.from_data_frame(val_df, cat_flds,val_y), bs, test_ds=test_ds)\n",
    "\n",
    "    @classmethod\n",
    "    def from_data_frame(cls, path, val_idxs, trn_idxs, df, y, cat_flds, bs, test_df=None):\n",
    "        val_df, val_y = df.iloc[val_idxs], y[val_idxs]\n",
    "        trn_df, trn_y = df.iloc[trn_idxs], y[trn_idxs]\n",
    "        return cls.from_data_frames(path, trn_df, val_df, trn_y, val_y, cat_flds, bs, test_df)\n",
    "    \n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops, y_range=None, use_bn=False, classify=None):\n",
    "        super().__init__() ## inherit from nn.Module parent class\n",
    "        self.embs = nn.ModuleList([nn.Embedding(m, d) for m, d in emb_szs]) ## construct embeddings\n",
    "        for emb in self.embs: emb_init(emb) ## initialize embedding weights\n",
    "        n_emb = sum(e.embedding_dim for e in self.embs) ## get embedding dimension needed for 1st layer\n",
    "        szs = [n_emb+n_cont] + szs ## add input layer to szs\n",
    "        self.lins = nn.ModuleList([\n",
    "            nn.Linear(szs[i], szs[i+1]) for i in range(len(szs)-1)]) ## create linear layers input, l1 -> l1, l2 ...\n",
    "        self.bns = nn.ModuleList([\n",
    "            nn.BatchNorm1d(sz) for sz in szs[1:]]) ## batchnormalization for hidden layers activations\n",
    "        for o in self.lins: kaiming_normal(o.weight.data) ## init weights with kaiming normalization\n",
    "        self.outp = nn.Linear(szs[-1], out_sz) ## create linear from last hidden layer to output\n",
    "        kaiming_normal(self.outp.weight.data) ## do kaiming initialization\n",
    "        \n",
    "        self.emb_drop = nn.Dropout(emb_drop) ## embedding dropout, will zero out weights of embeddings\n",
    "        self.drops = nn.ModuleList([nn.Dropout(drop) for drop in drops]) ## fc layer dropout\n",
    "        self.bn = nn.BatchNorm1d(n_cont) # bacthnorm for continous data\n",
    "        self.use_bn,self.y_range = use_bn,y_range \n",
    "        self.classify = classify\n",
    "        \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [emb(x_cat[:, i]) for i, emb in enumerate(self.embs)] # takes necessary emb vectors \n",
    "        x = torch.cat(x, 1) ## concatenate along axis = 1 (columns - side by side) # this is our input from cats\n",
    "        x = self.emb_drop(x) ## apply dropout to elements of embedding tensor\n",
    "        x2 = self.bn(x_cont) ## apply batchnorm to continous variables\n",
    "        x = torch.cat([x, x2], 1) ## concatenate cats and conts for final input\n",
    "        for l, d, b in zip(self.lins, self.drops, self.bns):\n",
    "            x = F.relu(l(x)) ## dotprod + non-linearity\n",
    "            if self.use_bn: x = b(x) ## apply batchnorm activations\n",
    "            x = d(x) ## apply dropout to activations\n",
    "        x = self.outp(x) # we defined this externally just not to apply dropout to output\n",
    "        if self.classify:\n",
    "            x = F.sigmoid(x) # for classification\n",
    "        elif y_range:\n",
    "            x = F.sigmoid(x) ## scales the output between 0,1\n",
    "            x = x*(self.y_range[1] - self.y_range[0]) ## scale output\n",
    "            x = x + self.y_range[0] ## shift output\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_test = pd.concat([train, val])[['holiday', 'weekday', 'hldy_seq', 'timeslot', 'TT_next30']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['holiday', 'weekday', 'hldy_seq', 'timeslot']\n",
    "train_test = preprocess(train_test, cats) # turn category into id\n",
    "# drop=False: the origin index will be added to the dataframe, but we don't need, so use drop=True\n",
    "train_test = train_test.reset_index(drop=True)\n",
    "train_test = EmbeddingDataPreprocess(train_test, cats, inplace=True) # turn id to another id (meaningless imo...)\n",
    "train_df = train_test.iloc[range(len(train))]\n",
    "test_df = train_test.iloc[range(len(train),len(train_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train\n",
    "test_id = test.index\n",
    "del val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingModel(\n",
       "  (embs): ModuleList(\n",
       "    (0): Embedding(9, 5)\n",
       "    (1): Embedding(7, 4)\n",
       "    (2): Embedding(9, 5)\n",
       "    (3): Embedding(288, 50)\n",
       "  )\n",
       "  (lins): ModuleList(\n",
       "    (0): Linear(in_features=65, out_features=1000)\n",
       "    (1): Linear(in_features=1000, out_features=500)\n",
       "  )\n",
       "  (bns): ModuleList(\n",
       "    (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True)\n",
       "  )\n",
       "  (outp): Linear(in_features=500, out_features=1)\n",
       "  (emb_drop): Dropout(p=0.04)\n",
       "  (drops): ModuleList(\n",
       "    (0): Dropout(p=0.001)\n",
       "    (1): Dropout(p=0.01)\n",
       "  )\n",
       "  (bn): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input, train_y = train_df.drop('TT_next30', 1), np.log(train_df['TT_next30'] + 1)\n",
    "test_input, test_y = test_df.drop('TT_next30', 1), np.log(test_df['TT_next30'] + 1)\n",
    "y_range = (train_y.min(), train_y.max())\n",
    "emb_szs = get_embs_dims(train_test, cats)\n",
    "\n",
    "model_data = EmbeddingModelData.from_data_frames('./tmp', train_input, test_input, train_y, test_y, cats, bs=32) \n",
    "emb_model = EmbeddingModel(emb_szs, 1, 0.04, 1, [1000, 500], [0.001, 0.01], y_range = y_range, classify=None)\n",
    "emb_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_train(model, model_data, optimizer, criterion, epochs):    \n",
    "    for epoch in range(epochs):\n",
    "        for data in iter(model_data.trn_dl):\n",
    "            \n",
    "            # get inputs\n",
    "            x_cats, x_conts, y = data\n",
    "\n",
    "            # wrap with variable\n",
    "            x_cats, x_conts, y = Variable(x_cats), Variable(x_conts), Variable(y)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = model(x_cats, x_conts)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-fdaa85751894>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcrit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0membedding_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-69-57044769adb8>\u001b[0m in \u001b[0;36membedding_train\u001b[1;34m(model, model_data, optimizer, criterion, epochs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0membedding_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrn_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[1;31m# get inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\gf\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\gf\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m  \u001b[1;31m# ensure that the worker exits on process exit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m                 \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\gf\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\gf\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\gf\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\gf\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\gf\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "opt = optim.SGD(emb_model.parameters(), lr = 1e-4, weight_decay=1e-4)\n",
    "crit = F.mse_loss\n",
    "epochs = 1\n",
    "embedding_train(emb_model, model_data, opt, crit, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second training\n",
    "opt = optim.SGD(emb_model.parameters(), lr = 5e-4, weight_decay=1e-4)\n",
    "crit = F.mse_loss\n",
    "epochs = 1\n",
    "embedding_train(emb_model, model_data, opt, crit, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
